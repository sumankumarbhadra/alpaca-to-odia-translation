{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYQKVZgs0ssV"
      },
      "source": [
        "## Alpaca to Odia Translation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZeabLSHj38WU"
      },
      "source": [
        "#Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bF4cZ7zS4M9a"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!git clone https://github.com/AI4Bharat/IndicTrans2.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mLGukTK64O1S"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "%cd /content/IndicTrans2/huggingface_interface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mLrtcXL54QTT"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!python3 -m pip install nltk sacremoses pandas regex mock transformers>=4.33.2 mosestokenizer\n",
        "!python3 -c \"import nltk; nltk.download('punkt')\"\n",
        "!python3 -m pip install bitsandbytes scipy accelerate datasets\n",
        "!python3 -m pip install sentencepiece\n",
        "\n",
        "!git clone https://github.com/VarunGumma/IndicTransToolkit.git\n",
        "%cd IndicTransToolkit\n",
        "!python3 -m pip install --editable ./\n",
        "%cd .."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZJk83Vu4S6Y"
      },
      "source": [
        "**IMPORTANT : Restart your run-time first and then run the cells below.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EyPfH7Rw4aj7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForSeq2SeqLM, BitsAndBytesConfig, AutoTokenizer\n",
        "from IndicTransToolkit import IndicProcessor\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "quantization = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lugINMiZ44er"
      },
      "outputs": [],
      "source": [
        "def initialize_model_and_tokenizer(ckpt_dir, quantization):\n",
        "    if quantization == \"4-bit\":\n",
        "        qconfig = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_use_double_quant=True,\n",
        "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "        )\n",
        "    elif quantization == \"8-bit\":\n",
        "        qconfig = BitsAndBytesConfig(\n",
        "            load_in_8bit=True,\n",
        "            bnb_8bit_use_double_quant=True,\n",
        "            bnb_8bit_compute_dtype=torch.bfloat16,\n",
        "        )\n",
        "    else:\n",
        "        qconfig = None\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(ckpt_dir, trust_remote_code=True)\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "        ckpt_dir,\n",
        "        trust_remote_code=True,\n",
        "        low_cpu_mem_usage=True,\n",
        "        quantization_config=qconfig,\n",
        "    )\n",
        "\n",
        "    if qconfig == None:\n",
        "        model = model.to(DEVICE)\n",
        "        if DEVICE == \"cuda\":\n",
        "            model.half()\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    return tokenizer, model\n",
        "\n",
        "\n",
        "def batch_translate(input_sentences, src_lang, tgt_lang, model, tokenizer, ip):\n",
        "    translations = []\n",
        "    for i in range(0, len(input_sentences), BATCH_SIZE):\n",
        "        batch = input_sentences[i : i + BATCH_SIZE]\n",
        "\n",
        "        # Preprocess the batch and extract entity mappings\n",
        "        batch = ip.preprocess_batch(batch, src_lang=src_lang, tgt_lang=tgt_lang)\n",
        "\n",
        "        # Tokenize the batch and generate input encodings\n",
        "        inputs = tokenizer(\n",
        "            batch,\n",
        "            truncation=True,\n",
        "            padding=\"longest\",\n",
        "            return_tensors=\"pt\",\n",
        "            return_attention_mask=True,\n",
        "        ).to(DEVICE)\n",
        "\n",
        "        # Generate translations using the model\n",
        "        with torch.no_grad():\n",
        "            generated_tokens = model.generate(\n",
        "                **inputs,\n",
        "                use_cache=True,\n",
        "                min_length=0,\n",
        "                max_length=256,\n",
        "                num_beams=5,\n",
        "                num_return_sequences=1,\n",
        "            )\n",
        "\n",
        "        # Decode the generated tokens into text\n",
        "\n",
        "        with tokenizer.as_target_tokenizer():\n",
        "            generated_tokens = tokenizer.batch_decode(\n",
        "                generated_tokens.detach().cpu().tolist(),\n",
        "                skip_special_tokens=True,\n",
        "                clean_up_tokenization_spaces=True,\n",
        "            )\n",
        "\n",
        "        # Postprocess the translations, including entity replacement\n",
        "        translations += ip.postprocess_batch(generated_tokens, lang=tgt_lang)\n",
        "\n",
        "        del inputs\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    return translations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qCz8BcV05B6-"
      },
      "outputs": [],
      "source": [
        "en_indic_ckpt_dir = \"ai4bharat/indictrans2-en-indic-1B\"\n",
        "en_indic_tokenizer, en_indic_model = initialize_model_and_tokenizer(en_indic_ckpt_dir, quantization)\n",
        "\n",
        "ip = IndicProcessor(inference=True)\n",
        "\n",
        "src_lang, tgt_lang = \"eng_Latn\", \"ory_Orya\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FgVhRt7o0r4V"
      },
      "outputs": [],
      "source": [
        "!pip install datasets sentencepiece huggingface_hub -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xizIDf_T01Vs"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import json\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "from datasets import load_dataset\n",
        "from huggingface_hub import login"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgXcbeQpJZAw"
      },
      "source": [
        "# Download and prepare the Alpaca dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b-f2UMVhJccr"
      },
      "outputs": [],
      "source": [
        "print(\"Loading Alpaca dataset...\")\n",
        "!wget -q https://raw.githubusercontent.com/tatsu-lab/stanford_alpaca/main/alpaca_data.json\n",
        "\n",
        "with open('alpaca_data.json', 'r') as f:\n",
        "    alpaca_data = json.load(f)\n",
        "\n",
        "print(f\"Loaded {len(alpaca_data)} examples from Alpaca dataset\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJ5mYpgi1VIw"
      },
      "source": [
        "# Process the Alpaca dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "5444Dyk01V8p"
      },
      "outputs": [],
      "source": [
        "print(\"Processing Alpaca dataset for translation...\")\n",
        "instructions = [item[\"instruction\"] for item in alpaca_data]\n",
        "inputs = [item[\"input\"] if item[\"input\"] else \"\" for item in alpaca_data]\n",
        "outputs = [item[\"output\"] for item in alpaca_data]\n",
        "\n",
        "# Translating in parts\n",
        "print(\"Translating instructions...\")\n",
        "odia_instructions = batch_translate(instructions, src_lang, tgt_lang, en_indic_model, en_indic_tokenizer, ip)\n",
        "\n",
        "\n",
        "print(\"Translating inputs...\")\n",
        "odia_inputs = batch_translate(inputs, src_lang, tgt_lang, en_indic_model, en_indic_tokenizer, ip)\n",
        "\n",
        "\n",
        "print(\"Translating outputs...\")\n",
        "odia_outputs = batch_translate(outputs, src_lang, tgt_lang, en_indic_model, en_indic_tokenizer, ip)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yuZ6nhUp2N1n"
      },
      "source": [
        "# Create Odia Alpaca dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v1IaAGRx2OHL"
      },
      "outputs": [],
      "source": [
        "odia_alpaca_data = []\n",
        "for i in range(len(alpaca_data)):\n",
        "    odia_alpaca_data.append({\n",
        "        \"instruction\": odia_instructions[i],\n",
        "        \"input\": odia_inputs[i],\n",
        "        \"output\": odia_outputs[i],\n",
        "        \"original_instruction\": instructions[i],\n",
        "        \"original_input\": inputs[i],\n",
        "        \"original_output\": outputs[i]\n",
        "    })\n",
        "\n",
        "# Save the dataset\n",
        "with open('alpaca_odia_data.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(odia_alpaca_data, f, ensure_ascii=False, indent=4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jd9KmNaX2UoK"
      },
      "outputs": [],
      "source": [
        "\n",
        "print(\"Sample for verification...\")\n",
        "# Few samples for manual verification\n",
        "samples = pd.DataFrame(odia_alpaca_data[:10])\n",
        "samples.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5HuxVVK2mTq"
      },
      "source": [
        "# Create HuggingFace Dataset and push to Hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mlsErVm32kic"
      },
      "outputs": [],
      "source": [
        "print(\"Preparing dataset for HuggingFace...\")\n",
        "from datasets import Dataset\n",
        "\n",
        "dataset = Dataset.from_pandas(pd.DataFrame(odia_alpaca_data))\n",
        "\n",
        "# Save dataset to Hub\n",
        "dataset_name = \"sumankumarbhadra/alpaca-odia\"\n",
        "dataset.push_to_hub(\n",
        "    dataset_name,\n",
        "    private=False,\n",
        "    token=hf_token\n",
        ")\n",
        "\n",
        "print(f\"Dataset uploaded to HuggingFace Hub as '{dataset_name}'\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}